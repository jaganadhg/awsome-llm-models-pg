{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightining IR Create Custom Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, BatchEncoding\n",
    "\n",
    "from lightning_ir import (\n",
    "    CrossEncoderModel,\n",
    "    CrossEncoderModule,\n",
    "    CrossEncoderOutput,\n",
    "    CrossEncoderTokenizer,\n",
    "    LightningIRDataModule,\n",
    "    LightningIRTrainer,\n",
    "    RankNet,\n",
    "    TupleDataset,\n",
    ")\n",
    "from lightning_ir.cross_encoder.config import CrossEncoderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoCrossEncoderConfig(CrossEncoderConfig):\n",
    "    model_type = \"custom-cross-encoder\"\n",
    "\n",
    "    ADDED_ARGS = CrossEncoderConfig.ADDED_ARGS.union({\"additional_linear_layer\"})\n",
    "\n",
    "    def __init__(self, additional_linear_layer: bool = True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.additional_linear_layer = additional_linear_layer\n",
    "\n",
    "\n",
    "class DemoCrossEncoderModel(CrossEncoderModel):\n",
    "    config_class = DemoCrossEncoderConfig\n",
    "\n",
    "    def __init__(self, config: DemoCrossEncoderConfig, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.additional_linear_layer = None\n",
    "        if config.additional_linear_layer:\n",
    "            self.additional_linear_layer = torch.nn.Linear(\n",
    "                config.hidden_size, config.hidden_size\n",
    "            )\n",
    "\n",
    "    def forward(self, encoding: BatchEncoding) -> torch.Tensor:\n",
    "        embeddings = self._backbone_forward(**encoding).last_hidden_state\n",
    "        embeddings = self._pooling(\n",
    "            embeddings,\n",
    "            encoding.get(\"attention_mask\", None),\n",
    "            pooling_strategy=self.config.pooling_strategy,\n",
    "        )\n",
    "        if self.additional_linear_layer is not None:\n",
    "            embeddings = self.additional_linear_layer(embeddings)\n",
    "        scores = self.linear(embeddings).view(-1)\n",
    "        return CrossEncoderOutput(scores=scores, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoConfig.register(DemoCrossEncoderConfig.model_type, DemoCrossEncoderConfig)\n",
    "AutoModel.register(DemoCrossEncoderConfig, DemoCrossEncoderModel)\n",
    "AutoTokenizer.register(DemoCrossEncoderConfig, CrossEncoderTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type bert-custom-cross-encoder. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of CustomCrossEncoderBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.additional_linear_layer.bias', 'bert.additional_linear_layer.weight', 'bert.linear.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'CustomCrossEncoderBertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "module = CrossEncoderModule(\n",
    "    model_name_or_path=\"bert-base-uncased\",\n",
    "    config=DemoCrossEncoderConfig(),  # our custom config\n",
    "    loss_functions=[RankNet()],\n",
    ")\n",
    "module.set_optimizer(AdamW, lr=1e-5)\n",
    "data_module = LightningIRDataModule(\n",
    "    train_dataset=TupleDataset(\"msmarco-passage/train/triples-small\"),\n",
    "    train_batch_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                        | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model | CustomCrossEncoderBertModel | 109 M  | train\n",
      "--------------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.932   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 100/? [00:35<00:00,  2.83it/s, v_num=15, loss=0.0107]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 100/? [01:04<00:00,  1.55it/s, v_num=15, loss=0.0107]\n"
     ]
    }
   ],
   "source": [
    "trainer = LightningIRTrainer(max_steps=100, max_epochs=3, accelerator=device_str)\n",
    "trainer.fit(module, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"custom-cross-encoder.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method save_checkpoint in module lightning.pytorch.trainer.trainer:\n",
      "\n",
      "save_checkpoint(filepath: Union[str, pathlib.Path], weights_only: bool = False, storage_options: Optional[Any] = None) -> None method of lightning_ir.main.LightningIRTrainer instance\n",
      "    Runs routine to create a checkpoint.\n",
      "    \n",
      "    This method needs to be called on all processes in case the selected strategy is handling distributed\n",
      "    checkpointing.\n",
      "    \n",
      "    Args:\n",
      "        filepath: Path where checkpoint is saved.\n",
      "        weights_only: If ``True``, will only save the model weights.\n",
      "        storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\n",
      "    \n",
      "    Raises:\n",
      "        AttributeError:\n",
      "            If the model is not attached to the Trainer before calling this method.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trainer.save_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
